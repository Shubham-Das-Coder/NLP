{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871a5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efdbbff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"ita.txt\", 'r', encoding = \"utf8\")\n",
    "raw_data = []\n",
    "\n",
    "for line in file:\n",
    "    pos = line.find(\"CC-BY\")\n",
    "    line = line[:pos-1]\n",
    "    \n",
    "    # Split the data into english and Italian\n",
    "    eng, ita = line.split('\\t')\n",
    "    \n",
    "    # form tuples of the data\n",
    "    data = eng, ita\n",
    "    raw_data.append(data)\n",
    "    \n",
    "file.close()\n",
    "\n",
    "def convert(list): \n",
    "    return tuple(list) \n",
    "  \n",
    "data = convert(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa7a5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"ita.txt\", 'r', encoding = \"utf8\")\n",
    "raw_data = []\n",
    "\n",
    "for line in file:\n",
    "    pos = line.find(\"CC-BY\")\n",
    "    line = line[:pos-1]\n",
    "    \n",
    "    # Split the data into english and Italian\n",
    "    eng, ita = line.split('\\t')\n",
    "    \n",
    "    # form tuples of the data\n",
    "    data = eng, ita\n",
    "    raw_data.append(data)\n",
    "    \n",
    "file.close()\n",
    "\n",
    "def convert(list): \n",
    "    return tuple(list) \n",
    "  \n",
    "data = convert(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e24260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower())\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "\n",
    "    s = s.strip()\n",
    "    s = '<start>' +' '+ s +' '+' <end>'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca596506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiting the data and Splitting into seperate lists and add tokens\n",
    "\n",
    "data = data[:30000]\n",
    "\n",
    "lang_eng = []\n",
    "lang_ita = []\n",
    "\n",
    "raw_data_en, raw_data_ita = list(zip(*data))\n",
    "raw_data_en, raw_data_ita = list(raw_data_en), list(raw_data_ita)\n",
    "\n",
    "for i, j in zip(raw_data_en, raw_data_ita):\n",
    "  preprocessed_data_en = preprocess_sentence(i)\n",
    "  preprocessed_data_ita = preprocess_sentence(j)\n",
    "  lang_eng.append(preprocessed_data_en)\n",
    "  lang_ita.append(preprocessed_data_ita)\n",
    "\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer\n",
    "\n",
    "input_tensor, inp_lang = tokenize(lang_ita)\n",
    "target_tensor, targ_lang = tokenize(lang_eng)\n",
    "\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91d6611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n",
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "7 ----> io\n",
      "9 ----> non\n",
      "11 ----> ho\n",
      "1058 ----> sonno\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "4 ----> i\n",
      "9 ----> m\n",
      "34 ----> not\n",
      "462 ----> sleepy\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
    "\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "\n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad994148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(64, 11), dtype=tf.int32, name=None), TensorSpec(shape=(64, 8), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e17267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, inp_vocab_size, embedding_size, lstm_size, input_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Encoder LSTM layer\n",
    "        \n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = tf.keras.layers.Embedding(inp_vocab_size, embedding_size)\n",
    "        self.lstm = tf.keras.layers.LSTM(lstm_size, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, input_sequence, states):\n",
    "      \n",
    "        embed = self.embedding(input_sequence)\n",
    "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
    "\n",
    "        return output, state_h, state_c\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "    \n",
    "        return (tf.zeros([batch_size, self.lstm_size]),\n",
    "                tf.zeros([batch_size, self.lstm_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "815887f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,scoring_function, att_units):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.scoring_function = scoring_function\n",
    "        self.att_units = att_units\n",
    "\n",
    "        if self.scoring_function=='dot':\n",
    "            pass\n",
    "            # For general, it would be self.wa = tf.keras.layers.Dense(att_units)\n",
    "\n",
    "\n",
    "    def call(self,decoder_hidden_state,encoder_output):\n",
    "\n",
    "        if self.scoring_function == 'dot':\n",
    "            \n",
    "            new_state = tf.expand_dims(decoder_hidden_state, -1)\n",
    "            score = tf.matmul(encoder_output, new_state)\n",
    "            weights = tf.nn.softmax(score, axis=1)\n",
    "            context = weights * encoder_output\n",
    "            context_vector = tf.reduce_sum(context, axis=1)\n",
    "                                \n",
    "            return context_vector, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2971910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.Model):\n",
    "    def __init__(self, tar_vocab_size, embedding_dim, input_length, dec_units, score_fun, att_units):\n",
    "        super(One_Step_Decoder, self).__init__()\n",
    "        # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "        self.tar_vocab_size = tar_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.dec_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "        self.embedding = tf.keras.layers.Embedding(self.tar_vocab_size, self.embedding_dim, \n",
    "                                                   input_length=self.input_length)\n",
    "        \n",
    "        self.lstm = tf.keras.layers.LSTM(self.dec_units, return_sequences=True, \n",
    "                                         return_state=True)\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(self.tar_vocab_size)\n",
    "        \n",
    "        self.attention = Attention(self.score_fun, self.att_units)\n",
    "\n",
    "    def call(self, input_to_decoder, encoder_output, state_h, state_c):\n",
    "        \n",
    "        result = self.embedding(input_to_decoder)\n",
    "        \n",
    "        context_vector, weights = self.attention(state_h, encoder_output)\n",
    "        \n",
    "        concat = tf.concat([tf.expand_dims(context_vector, 1), result], axis=-1)\n",
    "        \n",
    "        decoder_output, hidden_state, cell_state = self.lstm(concat, initial_state=[state_h, state_c])\n",
    "        \n",
    "        final_output = tf.reshape(decoder_output, (-1, decoder_output.shape[2]))\n",
    "        final_output = self.output_layer(final_output)\n",
    "        \n",
    "        return final_output, hidden_state, cell_state, weights, context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "967f05fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, out_vocab_size, embedding_dim, output_length, dec_units ,score_fun ,att_units):\n",
    "        #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "        super(Decoder, self).__init__()\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_length = output_length\n",
    "        self.dec_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "        self.onestepdecoder = One_Step_Decoder(self.out_vocab_size, self.embedding_dim, self.output_length,\n",
    "                                               self.dec_units, self.score_fun, self.att_units)\n",
    "        \n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state):\n",
    "        \n",
    "        all_outputs= tf.TensorArray(tf.float32, size=input_to_decoder.shape[1], name=\"output_arrays\")\n",
    "        \n",
    "        \n",
    "        for timestep in range(input_to_decoder.shape[1]):\n",
    "            output, decoder_hidden_state, decoder_cell_state, weights, context_vector = self.onestepdecoder(\n",
    "                                                                                    input_to_decoder[:,timestep:timestep+1], \n",
    "                                                                                    encoder_output, \n",
    "                                                                                    decoder_hidden_state,\n",
    "                                                                                    decoder_cell_state)\n",
    "            \n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        \n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0, 2)) \n",
    "\n",
    "        return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3178cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_size, lstm_size, \n",
    "                 input_length, output_length, dec_units ,score_fun ,att_units, batch_size):\n",
    "        \n",
    "        super(encoder_decoder, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_size, lstm_size, input_length)\n",
    "        self.decoder = Decoder(out_vocab_size, embedding_size, output_length, \n",
    "                               dec_units, score_fun, att_units)\n",
    "    \n",
    "    def call(self, data):\n",
    "        \n",
    "        input_sequence, input_to_decoder = data[0],data[1]\n",
    "        initial_state = self.encoder.initialize_states(batch_size=64)\n",
    "        encoder_output, state_h, state_c = self.encoder(input_sequence, initial_state)\n",
    "        decoder_hidden_state = state_h\n",
    "        decoder_cell_state = state_c\n",
    "        decoder_output = self.decoder(input_to_decoder, encoder_output, decoder_hidden_state, decoder_cell_state)\n",
    "        \n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba7ee5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a28d1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file logs already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir logs\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"dot.h5\", monitor='val_loss', verbose=1, save_weights_only=True)\n",
    "\n",
    "logdir='logs'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)\n",
    "\n",
    "input_vocab_size = len(inp_lang.word_index)+1\n",
    "output_vocab_size = len(targ_lang.word_index)+1\n",
    "\n",
    "input_len = max_length_inp\n",
    "output_len = max_length_targ\n",
    "\n",
    "lstm_size = 128\n",
    "att_units = 256\n",
    "dec_units = 128\n",
    "embedding_size = 300\n",
    "embedding_dim = 300\n",
    "score_fun = 'dot'\n",
    "steps = len(input_tensor)//64\n",
    "batch_size=64\n",
    "\n",
    "model = encoder_decoder(input_vocab_size,output_vocab_size,embedding_size,lstm_size,input_len,output_len,dec_units,score_fun,att_units, batch_size)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=model.layers[0],\n",
    "                                 decoder=model.layers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5cb6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden,enc_state = model.layers[0](inp, enc_hidden)\n",
    "\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      predictions = model.layers[1](dec_input,enc_output,enc_hidden,enc_state)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = model.layers[0].trainable_variables + model.layers[1].trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a42adcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 5.1854\n",
      "Epoch 1 Batch 100 Loss 2.9800\n",
      "Epoch 1 Batch 200 Loss 2.6107\n",
      "Epoch 1 Batch 300 Loss 2.3368\n",
      "Epoch 1 Loss 2.7550\n",
      "Time taken for 1 epoch 45.36477446556091 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.0934\n",
      "Epoch 2 Batch 100 Loss 2.1578\n",
      "Epoch 2 Batch 200 Loss 2.0111\n",
      "Epoch 2 Batch 300 Loss 2.0594\n",
      "Epoch 2 Loss 2.0596\n",
      "Time taken for 1 epoch 32.78007483482361 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.0198\n",
      "Epoch 3 Batch 100 Loss 2.0017\n",
      "Epoch 3 Batch 200 Loss 1.8402\n",
      "Epoch 3 Batch 300 Loss 1.9705\n",
      "Epoch 3 Loss 1.9219\n",
      "Time taken for 1 epoch 30.668047189712524 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.9197\n",
      "Epoch 4 Batch 100 Loss 1.9098\n",
      "Epoch 4 Batch 200 Loss 1.8424\n",
      "Epoch 4 Batch 300 Loss 1.7090\n",
      "Epoch 4 Loss 1.8158\n",
      "Time taken for 1 epoch 30.745992422103882 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.7111\n",
      "Epoch 5 Batch 100 Loss 1.5758\n",
      "Epoch 5 Batch 200 Loss 1.5924\n",
      "Epoch 5 Batch 300 Loss 1.5249\n",
      "Epoch 5 Loss 1.5797\n",
      "Time taken for 1 epoch 30.537801027297974 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.4773\n",
      "Epoch 6 Batch 100 Loss 1.4296\n",
      "Epoch 6 Batch 200 Loss 1.3780\n",
      "Epoch 6 Batch 300 Loss 1.3016\n",
      "Epoch 6 Loss 1.3849\n",
      "Time taken for 1 epoch 32.87994885444641 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.3026\n",
      "Epoch 7 Batch 100 Loss 1.2424\n",
      "Epoch 7 Batch 200 Loss 1.1477\n",
      "Epoch 7 Batch 300 Loss 1.1694\n",
      "Epoch 7 Loss 1.2287\n",
      "Time taken for 1 epoch 30.61618661880493 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.1598\n",
      "Epoch 8 Batch 100 Loss 1.0594\n",
      "Epoch 8 Batch 200 Loss 1.0683\n",
      "Epoch 8 Batch 300 Loss 1.0716\n",
      "Epoch 8 Loss 1.0926\n",
      "Time taken for 1 epoch 30.661961793899536 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.0149\n",
      "Epoch 9 Batch 100 Loss 1.0189\n",
      "Epoch 9 Batch 200 Loss 0.9925\n",
      "Epoch 9 Batch 300 Loss 0.9491\n",
      "Epoch 9 Loss 0.9692\n",
      "Time taken for 1 epoch 32.505730867385864 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.8912\n",
      "Epoch 10 Batch 100 Loss 0.8192\n",
      "Epoch 10 Batch 200 Loss 0.8335\n",
      "Epoch 10 Batch 300 Loss 0.8271\n",
      "Epoch 10 Loss 0.8540\n",
      "Time taken for 1 epoch 32.7993745803833 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.7898\n",
      "Epoch 11 Batch 100 Loss 0.7267\n",
      "Epoch 11 Batch 200 Loss 0.7605\n",
      "Epoch 11 Batch 300 Loss 0.7208\n",
      "Epoch 11 Loss 0.7490\n",
      "Time taken for 1 epoch 34.6742308139801 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.7197\n",
      "Epoch 12 Batch 100 Loss 0.7466\n",
      "Epoch 12 Batch 200 Loss 0.6784\n",
      "Epoch 12 Batch 300 Loss 0.6561\n",
      "Epoch 12 Loss 0.6528\n",
      "Time taken for 1 epoch 34.406267404556274 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.5734\n",
      "Epoch 13 Batch 100 Loss 0.5367\n",
      "Epoch 13 Batch 200 Loss 0.5869\n",
      "Epoch 13 Batch 300 Loss 0.5543\n",
      "Epoch 13 Loss 0.5640\n",
      "Time taken for 1 epoch 30.297157287597656 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.4612\n",
      "Epoch 14 Batch 100 Loss 0.5222\n",
      "Epoch 14 Batch 200 Loss 0.5564\n",
      "Epoch 14 Batch 300 Loss 0.4960\n",
      "Epoch 14 Loss 0.4833\n",
      "Time taken for 1 epoch 31.71528196334839 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.4303\n",
      "Epoch 15 Batch 100 Loss 0.4723\n",
      "Epoch 15 Batch 200 Loss 0.4360\n",
      "Epoch 15 Batch 300 Loss 0.3762\n",
      "Epoch 15 Loss 0.4102\n",
      "Time taken for 1 epoch 30.368193864822388 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.3692\n",
      "Epoch 16 Batch 100 Loss 0.3243\n",
      "Epoch 16 Batch 200 Loss 0.4222\n",
      "Epoch 16 Batch 300 Loss 0.3281\n",
      "Epoch 16 Loss 0.3447\n",
      "Time taken for 1 epoch 33.78756523132324 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.2565\n",
      "Epoch 17 Batch 100 Loss 0.2994\n",
      "Epoch 17 Batch 200 Loss 0.2605\n",
      "Epoch 17 Batch 300 Loss 0.2805\n",
      "Epoch 17 Loss 0.2909\n",
      "Time taken for 1 epoch 32.769407749176025 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.2165\n",
      "Epoch 18 Batch 100 Loss 0.2231\n",
      "Epoch 18 Batch 200 Loss 0.2357\n",
      "Epoch 18 Batch 300 Loss 0.2471\n",
      "Epoch 18 Loss 0.2450\n",
      "Time taken for 1 epoch 36.265782594680786 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.1813\n",
      "Epoch 19 Batch 100 Loss 0.2314\n",
      "Epoch 19 Batch 200 Loss 0.2002\n",
      "Epoch 19 Batch 300 Loss 0.2312\n",
      "Epoch 19 Loss 0.2071\n",
      "Time taken for 1 epoch 31.448416233062744 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.1681\n",
      "Epoch 20 Batch 100 Loss 0.2402\n",
      "Epoch 20 Batch 200 Loss 0.1784\n",
      "Epoch 20 Batch 300 Loss 0.1544\n",
      "Epoch 20 Loss 0.1761\n",
      "Time taken for 1 epoch 36.47686719894409 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = model.layers[0].initialize_states(64)\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "      \n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0c1e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "\n",
    "  attention_plot = np.zeros((output_len, input_len))\n",
    "\n",
    "  input_sentence = preprocess_sentence(input_sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in input_sentence.split()]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=input_len,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "  \n",
    "  encoder_output,state_h,state_c = model.layers[0](inputs,[tf.zeros((1, lstm_size)),tf.zeros((1, lstm_size))])\n",
    "\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(output_len):\n",
    "   predictions,state_h,state_c,attention_weights,context_vector = model.layers[1].onestepdecoder(dec_input,\n",
    "                                                                                                 encoder_output,\n",
    "                                                                                                 state_h,\n",
    "                                                                                                 state_c)\n",
    "\n",
    "   attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "   attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "   predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "   result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "   if targ_lang.index_word[predicted_id] == '<end>':\n",
    "     return result, input_sentence, attention_plot\n",
    "\n",
    "   dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, input_sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cf30224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sent, attention_plot = predict(sentence)\n",
    "\n",
    "  print('Input: %s' % (sent))\n",
    "  print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e881a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ciao !  <end>\n",
      "Predicted translation: hi . <end> \n"
     ]
    }
   ],
   "source": [
    "translate('ciao!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29dd5fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> buongiorno  <end>\n",
      "Predicted translation: are that aloud . <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Buongiorno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "304e291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> come stai ?  <end>\n",
      "Predicted translation: how are you ? <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Come stai?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aeadd76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> vorrei una birra  <end>\n",
      "Predicted translation: i like a know a fish like to \n"
     ]
    }
   ],
   "source": [
    "translate('Vorrei una birra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fca92400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> mi dispiace  <end>\n",
      "Predicted translation: i m the sun . <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Mi dispiace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
